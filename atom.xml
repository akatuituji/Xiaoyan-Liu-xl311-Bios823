<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://akatuituji.github.io</id>
    <title>Xiaoyan&apos;s Blog</title>
    <updated>2020-11-21T01:13:53.322Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://akatuituji.github.io"/>
    <link rel="self" href="https://akatuituji.github.io/atom.xml"/>
    <subtitle>For 823 homework&apos;s blogs</subtitle>
    <logo>https://akatuituji.github.io/images/avatar.png</logo>
    <icon>https://akatuituji.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Xiaoyan&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Mortality Prediction Models using MIMIC-III]]></title>
        <id>https://akatuituji.github.io/post/mortality-prediction-models-using-mimic-iii/</id>
        <link href="https://akatuituji.github.io/post/mortality-prediction-models-using-mimic-iii/">
        </link>
        <updated>2020-11-20T12:02:41.000Z</updated>
        <content type="html"><![CDATA[<p>The goal is to build a model framework to predict in-hospital mortality. The model would be useful to promptly identify high-risk patients who might be dead within hours or days since ICU admission, so that resources can be efficiently allocated during the early stage of ICU st.<br>
We referred to the instruction form MIMIC official website to set up our database in PostgreSQL. We firstly downloaded the whole dataset from the website. And then we created the mimiciii schema and then build a set of empty tables. After that we import the CSV data files into the empty tables. To interface with Python, we use psycopg2 to connect to database.<br>
In this project, to predict the mortality in the early stage of ICU stay, we are using three severity scores: SAPS II, APACHE II, and SOFA scores. For APACHE II [1], it is calculated from a patient's age and 12 routine physiological measurements. And measurements are measured during the first 24 hours after admission, and utilized in addition to information about previous health status (recent surgery, history of severe organ insufficiency, immunocompromised state) and baseline demographics such as age. Another key thing is that APACHE II only consider the most deranged physiology value physiology, which means we need to extract the minimum and maximum value with 24 hours.<br>
In order to extract the target variables, we first need a deep understanding on the data schema. According our observation, we found out that the main ICU admission data are included in table “icustays”. And the variable required to calculated these 3 score are distributed in “labevents” table and “chartevents” table. And these two tables connect to “icustay” through a key called subjectID. Table 2 provides the list of extracted features used in the study.<br>
<img src="https://akatuituji.github.io/post-images/1605921208070.png" alt="" loading="lazy"><br>
Next, a series of determining variables are collected when predicting survival time after discharge from hospital admission. The collected variables are extracted from queries to the database and in some cases to the preprocessing of these. The distribution in classes attends only to organizational criteria.</p>
<h2 id="age">Age:<img src="https://akatuituji.github.io/post-images/1605920832555.png" alt="" loading="lazy"></h2>
<h2 id="sex">Sex:<img src="https://akatuituji.github.io/post-images/1605920888585.png" alt="" loading="lazy"></h2>
<h2 id="marital-status">Marital status:<img src="https://akatuituji.github.io/post-images/1605920916598.png" alt="" loading="lazy"></h2>
<h2 id="physiological-signals">Physiological signals:<img src="https://akatuituji.github.io/post-images/1605921019145.png" alt="" loading="lazy"></h2>
<h2 id="length-of-stay-in-icu">Length of stay in ICU: <img src="https://akatuituji.github.io/post-images/1605921050393.png" alt="" loading="lazy"></h2>
<h2 id="probability-distribution-of-icu-stay">Probability distribution of ICU stay: <img src="https://akatuituji.github.io/post-images/1605921078516.png" alt="" loading="lazy"></h2>
<h2 id="probability-distribution-of-the-entire-hospital-stay">Probability distribution of the entire hospital stay:<img src="https://akatuituji.github.io/post-images/1605921112881.png" alt="" loading="lazy"></h2>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homework 1 Xiaoyan Liu's First Blog]]></title>
        <id>https://akatuituji.github.io/post/homework-1-xiaoyan-lius-first-blog/</id>
        <link href="https://akatuituji.github.io/post/homework-1-xiaoyan-lius-first-blog/">
        </link>
        <updated>2020-11-16T18:26:54.000Z</updated>
        <content type="html"><![CDATA[<h1 id="about-me">About me</h1>
<p>Hi, my name is Xiaoyan, a master student in ECE Program. This is my second year in Duke and I would like to find a job as a data scientist. I am glad to take Bios823 and hope to learn something that is useful for interview and my future work.<br>
<img src="https://akatuituji.github.io/post-images/1605738718196.png" alt="" loading="lazy"></p>
<h1 id="what-is-gridea">What is Gridea</h1>
<p>Girdea is a static blog writing client. The project URL in Github is here: https://github.com/getgridea/gridea. And the client looks like this: <img src="https://akatuituji.github.io/post-images/1605739179807.png" alt="" loading="lazy"><br>
To write a blog, you just need to Download the client, set your own repository related to the client, add a new blog and upload it. Most people would choose to use Jekyll or Hexo as a static templet and write their blogs. Compared with Jekyll or Hexo, Gridea is very easy to install, you don't need to set environment and install many things. Also, it is very easy to operate for a new blogger.</p>
<h1 id="how-to-write-this-blog">How to write this blog</h1>
<ol>
<li>Sign up in Github and get a user name</li>
<li>Create a new repository as a file to save contents of your blogs.<br>
<img src="https://akatuituji.github.io/post-images/1605740036070.png" alt="" loading="lazy"></li>
<li>Download Gridea client and let your repository connected with this client.<br>
<img src="https://akatuituji.github.io/post-images/1605740138573.png" alt="" loading="lazy"></li>
<li>Write your blog and upload it.<br>
<img src="https://akatuituji.github.io/post-images/1605740217936.png" alt="" loading="lazy"><br>
<img src="https://akatuituji.github.io/post-images/1605740224000.png" alt="" loading="lazy"></li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homework 7 Use deep learning model to classify insects]]></title>
        <id>https://akatuituji.github.io/post/homework-7-use-deep-learning-model-to-classify-insects/</id>
        <link href="https://akatuituji.github.io/post/homework-7-use-deep-learning-model-to-classify-insects/">
        </link>
        <updated>2020-11-15T04:38:16.000Z</updated>
        <content type="html"><![CDATA[<p>In this homework, We are required to train a deep learning model to classify beetles, cockroaches and dragonflies using these images. Note: Original images from https://www.insectimages.org/index.cfm. Blog about this, and explain how the neural network classified the images.</p>
<h1 id="dataset">Dataset</h1>
<p>The dataset includes three parts: beetles, cockroach and dragonfiles. And our task is to let the model tell which kind of insect it is when you choose an image.</p>
<h1 id="beetles">beetles</h1>
<figure data-type="image" tabindex="1"><img src="https://akatuituji.github.io/post-images/1605671375625.png" alt="" loading="lazy"></figure>
<h1 id="cockroach">cockroach</h1>
<figure data-type="image" tabindex="2"><img src="https://akatuituji.github.io/post-images/1605671383566.png" alt="" loading="lazy"></figure>
<h1 id="dragonfiles">dragonfiles</h1>
<figure data-type="image" tabindex="3"><img src="https://akatuituji.github.io/post-images/1605671441313.png" alt="" loading="lazy"></figure>
<h1 id="preparation">Preparation</h1>
<p>I use two functions to transfer the images to our train and test dataset. First we need to load these images to our jupyter notebook. After getting the path, we use another fuction to get the correct size and format as the input of our CNN.<br>
<img src="https://akatuituji.github.io/post-images/1605672038810.png" alt="" loading="lazy"><br>
<img src="https://akatuituji.github.io/post-images/1605672045083.png" alt="" loading="lazy"></p>
<h1 id="build-cnn">Build CNN</h1>
<p>Our CNN has four layers: One input layer, two hidden layers and one output layer.<br>
<img src="https://akatuituji.github.io/post-images/1605672426643.png" alt="" loading="lazy"><br>
And After training for 200 epochs, we get the accuracy of the model, which is 80.56%<br>
<img src="https://akatuituji.github.io/post-images/1605673981687.png" alt="" loading="lazy"></p>
<h1 id="how-the-nn-classified-the-images">How the NN classified the images</h1>
<p>The main feature of the CNN network is the use of convolutional layers. This actually simulates the human visual nerve. A single neuron can only respond to certain image features, such as horizontal or vertical edges. It is very simple in itself, but these simple neurons form a layer, and after there are enough layers, sufficient features can be obtained.<br>
The image of each insect has a value, and each value corresponds to a pixel value, and the magnitude of the value reflects the intensity of the pixel. By recognizing pixels, the CNN could classify the images.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homework 6 dashboard visualization of PhDs data]]></title>
        <id>https://akatuituji.github.io/post/homework-6-dashboard-visualization-of-phds-data/</id>
        <link href="https://akatuituji.github.io/post/homework-6-dashboard-visualization-of-phds-data/">
        </link>
        <updated>2020-11-05T22:54:52.000Z</updated>
        <content type="html"><![CDATA[<p>In this homework, we are required to download data of PhDs awarded in the US. And do some analysis in pandas. Then make a dashboard visualization of a few interesting aspects of the data using dash or streamlit.</p>
<h1 id="preparation">Preparation</h1>
<p>First, we need to pip the streamlit. So what is streamlit? Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps. Thus we can use it to create a dashboard and do the visualization.<br>
<img src="https://akatuituji.github.io/post-images/1604711305372.png" alt="" loading="lazy"><br>
And let‘s see the dataset  PhDs awarded in the US. These tables present detailed data on the demographic characteristics, educational history, sources of financial support, and postgraduation plans of doctorate recipients. Explore the Survey of Earned Doctorates data further via NCSES's interactive data tool.</p>
<h1 id="make-a-dashboard">Make a Dashboard</h1>
<p>The first step is to create a .py file and import the libraries we need.<br>
<img src="https://akatuituji.github.io/post-images/1604713492323.png" alt="" loading="lazy"><br>
Then we use pandas to make our dataframes suitable for visualization. We extract the column of year and let it be the x-axis and the other columns be y-axis.<br>
<img src="https://akatuituji.github.io/post-images/1604713697224.png" alt="" loading="lazy"><br>
Finally, we do the visualization.<br>
<img src="https://akatuituji.github.io/post-images/1604713990966.png" alt="" loading="lazy"><br>
And the code is below.<br>
<img src="https://akatuituji.github.io/post-images/1604714017628.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homework5 Star Wars People]]></title>
        <id>https://akatuituji.github.io/post/star-wars-people/</id>
        <link href="https://akatuituji.github.io/post/star-wars-people/">
        </link>
        <updated>2020-10-21T15:47:32.000Z</updated>
        <content type="html"><![CDATA[<p>For this homework, We need to Use the requests library, download all the people in the Star Wars universe using the Star Wars API (https://swapi.dev/documentation). Show the name of the oldest person (or robot or alien) and list the titles of all the films they appeared in.</p>
<ol>
<li>We use requests to get the dataset and transform to dataframe.<br>
<img src="https://akatuituji.github.io/post-images/1603425847853.png" alt="" loading="lazy"></li>
<li>We need to get the list of the 82 people. So we use a while loop to get the information of all people.<br>
<img src="https://akatuituji.github.io/post-images/1603426021398.png" alt="" loading="lazy"></li>
<li>We set the name as out table's index so that we can choose the people we want conveniently.<img src="https://akatuituji.github.io/post-images/1603426437860.png" alt="" loading="lazy"></li>
<li>Then we deal with the unknown value in birth_year column, and change all the birth_year value into float type so that we can do sorting. After sorting, we get the oldest person (or robot or alien) Yoda.<br>
<img src="https://akatuituji.github.io/post-images/1603426910503.png" alt="" loading="lazy"></li>
<li>Finally we list the titles of all the films they appeared in.<br>
<img src="https://akatuituji.github.io/post-images/1603426973165.png" alt="" loading="lazy"><br>
The code is in the file Homework-823/hw5.</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homework 4 Spotify Songs Data Set]]></title>
        <id>https://akatuituji.github.io/post/homework-4-spotify-songs-data-set/</id>
        <link href="https://akatuituji.github.io/post/homework-4-spotify-songs-data-set/">
        </link>
        <updated>2020-10-08T12:25:20.000Z</updated>
        <content type="html"><![CDATA[<p>For this homework, we have two steps to finish it. The first thing is to download the Spotify songs data set and create a SQLite3 schema to store this data in at least 3rd normal form (3NF). The other thing is to use an SQL query to find the names of all playlists that contain instrumentals.</p>
<ol>
<li>We get the CSV file and use pandas to make it 3rd normal form. The table has already been 1NF since each cell contains a single value, no repeating groups and it has a primary key. So we just make it 2NF and 3NF. In the case, note that (track_id, track_album_id and playlist_id) is a candidate key. However, track_album_name and track_album_release_date depend only on track_album_id and not on track_id. Also, playlist_name, playlist_genre and playlist_subgenre depend only on playlist_id and not on track_id, so this violates 2NF.<br>
<img src="https://akatuituji.github.io/post-images/1602187771064.png" alt="" loading="lazy"><br>
We choose album feature to do the normalization. We select these columns that violate 2NF as a new dataframe. And reset the index to make it continuous. Then we insert a new column which is the same as index. So we get the table of album.<br>
<img src="https://akatuituji.github.io/post-images/1602188066375.png" alt="" loading="lazy"><br>
We can do the same thing for the playlist feature.<br>
<img src="https://akatuituji.github.io/post-images/1602188180750.png" alt="" loading="lazy"><br>
Finally, we merge these three tables. We just use number to raplace the album and playlist features.</li>
<li>Next thing is to use an SQL query to find the names of all playlists that contain instrumentals. First we communicate with database from pandas<br>
<img src="https://akatuituji.github.io/post-images/1602188397949.png" alt="" loading="lazy"><br>
Then we use sql to select what we want.<img src="https://akatuituji.github.io/post-images/1602188591868.png" alt="" loading="lazy"><br>
Finally we get the table that includes the names of all playlists that contain instrumentals.<br>
<img src="https://akatuituji.github.io/post-images/1602188634708.png" alt="" loading="lazy"></li>
</ol>
]]></content>
    </entry>
</feed>