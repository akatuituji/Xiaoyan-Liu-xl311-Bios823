<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://akatuituji.github.io</id>
    <title>Xiaoyan&apos;s Blog</title>
    <updated>2020-11-18T04:33:15.046Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://akatuituji.github.io"/>
    <link rel="self" href="https://akatuituji.github.io/atom.xml"/>
    <subtitle>For 823 homework&apos;s blogs</subtitle>
    <logo>https://akatuituji.github.io/images/avatar.png</logo>
    <icon>https://akatuituji.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, Xiaoyan&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Homework 7 Use deep learning model to classify insects]]></title>
        <id>https://akatuituji.github.io/post/homework-7-use-deep-learning-model-to-classify-insects/</id>
        <link href="https://akatuituji.github.io/post/homework-7-use-deep-learning-model-to-classify-insects/">
        </link>
        <updated>2020-11-15T04:38:16.000Z</updated>
        <content type="html"><![CDATA[<p>In this homework, We are required to train a deep learning model to classify beetles, cockroaches and dragonflies using these images. Note: Original images from https://www.insectimages.org/index.cfm. Blog about this, and explain how the neural network classified the images.</p>
<h1 id="dataset">Dataset</h1>
<p>The dataset includes three parts: beetles, cockroach and dragonfiles. And our task is to let the model tell which kind of insect it is when you choose an image.</p>
<h1 id="beetles">beetles</h1>
<figure data-type="image" tabindex="1"><img src="https://akatuituji.github.io/post-images/1605671375625.png" alt="" loading="lazy"></figure>
<h1 id="cockroach">cockroach</h1>
<figure data-type="image" tabindex="2"><img src="https://akatuituji.github.io/post-images/1605671383566.png" alt="" loading="lazy"></figure>
<h1 id="dragonfiles">dragonfiles</h1>
<figure data-type="image" tabindex="3"><img src="https://akatuituji.github.io/post-images/1605671441313.png" alt="" loading="lazy"></figure>
<h1 id="preparation">Preparation</h1>
<p>I use two functions to transfer the images to our train and test dataset. First we need to load these images to our jupyter notebook. After getting the path, we use another fuction to get the correct size and format as the input of our CNN.<br>
<img src="https://akatuituji.github.io/post-images/1605672038810.png" alt="" loading="lazy"><br>
<img src="https://akatuituji.github.io/post-images/1605672045083.png" alt="" loading="lazy"></p>
<h1 id="build-cnn">Build CNN</h1>
<p>Our CNN has four layers: One input layer, two hidden layers and one output layer.<br>
<img src="https://akatuituji.github.io/post-images/1605672426643.png" alt="" loading="lazy"><br>
And After training for 200 epochs, we get the accuracy of the model, which is 80.56%<br>
<img src="https://akatuituji.github.io/post-images/1605673981687.png" alt="" loading="lazy"></p>
<h1 id="how-the-nn-classified-the-images">How the NN classified the images</h1>
<p>The main feature of the CNN network is the use of convolutional layers. This actually simulates the human visual nerve. A single neuron can only respond to certain image features, such as horizontal or vertical edges. It is very simple in itself, but these simple neurons form a layer, and after there are enough layers, sufficient features can be obtained.<br>
The image of each insect has a value, and each value corresponds to a pixel value, and the magnitude of the value reflects the intensity of the pixel. By recognizing pixels, the CNN could classify the images.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homework 6 dashboard visualization of PhDs data]]></title>
        <id>https://akatuituji.github.io/post/homework-6-dashboard-visualization-of-phds-data/</id>
        <link href="https://akatuituji.github.io/post/homework-6-dashboard-visualization-of-phds-data/">
        </link>
        <updated>2020-11-05T22:54:52.000Z</updated>
        <content type="html"><![CDATA[<p>In this homework, we are required to download data of PhDs awarded in the US. And do some analysis in pandas. Then make a dashboard visualization of a few interesting aspects of the data using dash or streamlit.</p>
<h1 id="preparation">Preparation</h1>
<p>First, we need to pip the streamlit. So what is streamlit? Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps. Thus we can use it to create a dashboard and do the visualization.<br>
<img src="https://akatuituji.github.io/post-images/1604711305372.png" alt="" loading="lazy"><br>
And let‘s see the dataset  PhDs awarded in the US. These tables present detailed data on the demographic characteristics, educational history, sources of financial support, and postgraduation plans of doctorate recipients. Explore the Survey of Earned Doctorates data further via NCSES's interactive data tool.</p>
<h1 id="make-a-dashboard">Make a Dashboard</h1>
<p>The first step is to create a .py file and import the libraries we need.<br>
<img src="https://akatuituji.github.io/post-images/1604713492323.png" alt="" loading="lazy"><br>
Then we use pandas to make our dataframes suitable for visualization. We extract the column of year and let it be the x-axis and the other columns be y-axis.<br>
<img src="https://akatuituji.github.io/post-images/1604713697224.png" alt="" loading="lazy"><br>
Finally, we do the visualization.<br>
<img src="https://akatuituji.github.io/post-images/1604713990966.png" alt="" loading="lazy"><br>
And the code is below.<br>
<img src="https://akatuituji.github.io/post-images/1604714017628.png" alt="" loading="lazy"></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homework5 Star Wars People]]></title>
        <id>https://akatuituji.github.io/post/star-wars-people/</id>
        <link href="https://akatuituji.github.io/post/star-wars-people/">
        </link>
        <updated>2020-10-21T15:47:32.000Z</updated>
        <content type="html"><![CDATA[<p>For this homework, We need to Use the requests library, download all the people in the Star Wars universe using the Star Wars API (https://swapi.dev/documentation). Show the name of the oldest person (or robot or alien) and list the titles of all the films they appeared in.</p>
<ol>
<li>We use requests to get the dataset and transform to dataframe.<br>
<img src="https://akatuituji.github.io/post-images/1603425847853.png" alt="" loading="lazy"></li>
<li>We need to get the list of the 82 people. So we use a while loop to get the information of all people.<br>
<img src="https://akatuituji.github.io/post-images/1603426021398.png" alt="" loading="lazy"></li>
<li>We set the name as out table's index so that we can choose the people we want conveniently.<img src="https://akatuituji.github.io/post-images/1603426437860.png" alt="" loading="lazy"></li>
<li>Then we deal with the unknown value in birth_year column, and change all the birth_year value into float type so that we can do sorting. After sorting, we get the oldest person (or robot or alien) Yoda.<br>
<img src="https://akatuituji.github.io/post-images/1603426910503.png" alt="" loading="lazy"></li>
<li>Finally we list the titles of all the films they appeared in.<br>
<img src="https://akatuituji.github.io/post-images/1603426973165.png" alt="" loading="lazy"><br>
The code is in the file Homework-823/hw5.</li>
</ol>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homework 4 Spotify Songs Data Set]]></title>
        <id>https://akatuituji.github.io/post/homework-4-spotify-songs-data-set/</id>
        <link href="https://akatuituji.github.io/post/homework-4-spotify-songs-data-set/">
        </link>
        <updated>2020-10-08T12:25:20.000Z</updated>
        <content type="html"><![CDATA[<p>For this homework, we have two steps to finish it. The first thing is to download the Spotify songs data set and create a SQLite3 schema to store this data in at least 3rd normal form (3NF). The other thing is to use an SQL query to find the names of all playlists that contain instrumentals.</p>
<ol>
<li>We get the CSV file and use pandas to make it 3rd normal form. The table has already been 1NF since each cell contains a single value, no repeating groups and it has a primary key. So we just make it 2NF and 3NF. In the case, note that (track_id, track_album_id and playlist_id) is a candidate key. However, track_album_name and track_album_release_date depend only on track_album_id and not on track_id. Also, playlist_name, playlist_genre and playlist_subgenre depend only on playlist_id and not on track_id, so this violates 2NF.<br>
<img src="https://akatuituji.github.io/post-images/1602187771064.png" alt="" loading="lazy"><br>
We choose album feature to do the normalization. We select these columns that violate 2NF as a new dataframe. And reset the index to make it continuous. Then we insert a new column which is the same as index. So we get the table of album.<br>
<img src="https://akatuituji.github.io/post-images/1602188066375.png" alt="" loading="lazy"><br>
We can do the same thing for the playlist feature.<br>
<img src="https://akatuituji.github.io/post-images/1602188180750.png" alt="" loading="lazy"><br>
Finally, we merge these three tables. We just use number to raplace the album and playlist features.</li>
<li>Next thing is to use an SQL query to find the names of all playlists that contain instrumentals. First we communicate with database from pandas<br>
<img src="https://akatuituji.github.io/post-images/1602188397949.png" alt="" loading="lazy"><br>
Then we use sql to select what we want.<img src="https://akatuituji.github.io/post-images/1602188591868.png" alt="" loading="lazy"><br>
Finally we get the table that includes the names of all playlists that contain instrumentals.<br>
<img src="https://akatuituji.github.io/post-images/1602188634708.png" alt="" loading="lazy"></li>
</ol>
]]></content>
    </entry>
</feed>